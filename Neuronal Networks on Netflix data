import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from wordcloud import WordCloud
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import tensorflow as tf
from scipy.sparse import csr_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dropout, Dense
from tensorflow.keras.callbacks import EarlyStopping
import nltk
from nltk.stem import WordNetLemmatizer
from scipy.sparse import hstack
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error


nltk.download('wordnet')
nltk.download('stopwords')

df = pd.read_csv('/kaggle/input/netflix-life-impact-dataset-nlid/Netflix Life Impact Dataset (NLID).csv')

df.columns = df.columns.str.strip()


print(df.isnull().sum())


sns.countplot(x='Genre', data=df, color='red')
plt.xticks(rotation=45)
plt.title('Frequency of Genres')
plt.show()


sns.heatmap(df.corr(numeric_only=True), cmap='viridis', annot=True)
plt.title('Correlation Matrix')
plt.show()


sns.boxplot(x='Genre', y='Average Rating', data=df)
plt.xticks(rotation=45)
plt.title('Average Rating by Genre')
plt.show()


text = " ".join(df['How Discovered'].dropna().astype(str).str.strip())
word_cloud = WordCloud(width=700, height=500, max_words=100, background_color='black').generate(text)
plt.figure(figsize=(10, 7))
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud: How Discovered')
plt.show()

# Textverarbeitung
text_review = df['Review Highlights'].astype(str)

def remove_stopwords(text):
    stop_words = set(stopwords.words('english')) 
    good_words = [word.lower() for word in str(text).split() if word.lower() not in stop_words]
    return " ".join(good_words)

def clean_text(text):
    text = re.sub(r'[^\w\s]', ' ', text) 
    text = re.sub(r'\d+', ' ', text) 
    text = re.sub(r'\s+', ' ', text).strip()  
    lemmatizer = WordNetLemmatizer()
    text = " ".join([lemmatizer.lemmatize(word) for word in text.split()])
    return text

output = text_review.apply(remove_stopwords)
cleaned_text = output.apply(clean_text)

# Skalierung
scaler = StandardScaler()


df['Suggested to Friends/Family'] = df['Suggested to Friends/Family (Y/N %)'].str.extract(r'(\d+)').astype(float)


vectorizer = TfidfVectorizer(max_features=5000)  
vector_text = vectorizer.fit_transform(cleaned_text)


df_numerics = df[['Suggested to Friends/Family', 'Number of Reviews', 'Release Year']].fillna(0)
numeric_features = scaler.fit_transform(df_numerics)


X = hstack([vector_text, numeric_features])  


y = df['Average Rating'].fillna(df['Average Rating'].mean())  

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


best_params_KNN = {'n_neighbors': [3, 5, 10, 20, 30, 50, 100]}
best_params_RFC = {'n_estimators': [10, 100, 200, 300]}
KNN = GridSearchCV(estimator=KNeighborsRegressor(), param_grid=best_params_KNN, cv=5, scoring='neg_mean_absolute_error')
RFC = GridSearchCV(estimator=RandomForestRegressor(random_state=42), param_grid=best_params_RFC, cv=5, scoring='neg_mean_absolute_error')
LR = LinearRegression()


models = [
    (KNN, 'KNeighborsRegressor'),
    (LR, 'LinearRegression'),
    (RFC, 'RandomForestRegressor'),
]


for model, name in models:
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    mae = mean_absolute_error(y_test, predictions)
    print(f'Model: {name}, MSE = {mse:.4f}, MAE = {mae:.4f}')


X_train_dense = X_train.toarray()
X_test_dense = X_test.toarray()
input_dim = X_train.shape[1]

nn_model = Sequential([
    Dense(128, activation='relu', input_shape=(input_dim,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1)
])

nn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])
nn_model.summary()

early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)

history = nn_model.fit(
    X_train_dense, y_train,
    epochs=100,
    batch_size=20,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)


loss, mae = nn_model.evaluate(X_test_dense, y_test, verbose=1)
predictions = nn_model.predict(X_test_dense).flatten()

mse = mean_squared_error(y_test, predictions)
mae = mean_absolute_error(y_test, predictions)  
print(f"Neural Network Test Mean Absolute Error (MAE): {mae:.4f}")
print(f"Neural Network Test Mean Squared Error (MSE): {mse:.4f}")


plt.figure(figsize=(12, 8))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Neural Network Training & Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.show()
